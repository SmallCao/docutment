 事件的信息量随着事件发生概率的变大而 递减，信息不为负  
 两个不相关事件同时发生产生的信息： h(x,y) = h(x) + h(y)  
 两个事件的概率满足： p(x,y) = p(x) * p(y).

对数形式的 真数相乘=>对数相加  

𝐡(𝐱) = −𝒍𝒐𝒈𝟐𝒑(𝒙)

![信息熵](res/entropy_1.png)

𝐟(𝐱) = −𝒍𝒐𝒈𝟐𝒙  函数图像
![-log2x](res/entropy_2.png)
